# iptv_scraper.py
import asyncio
from playwright.async_api import async_playwright
import re
import time
import requests
from urllib.parse import urlparse
import os

# ================== é…ç½®åŒºåŸŸï¼ˆæ— éœ€ä¿®æ”¹ï¼‰==================
SEARCH_URL = "http://tonkiang.us/"
TIMEOUT = 10  # æµ‹é€Ÿè¶…æ—¶
TARGET_CHANNELS = [
    "CCTV-1", "CCTV-2", "CCTV-3", "CCTV-4", "CCTV-5", "CCTV-5+", "CCTV-6",
    "CCTV-7", "CCTV-8", "CCTV-9", "CCTV-10", "CCTV-11", "CCTV-12", "CCTV-13",
    "CCTV-14", "CCTV-15", "CCTV-17", "CCTV-4K", "å‡¤å‡°å«è§†", "å‡¤å‡°ä¸­æ–‡å°", "å‡¤å‡°èµ„è®¯å°",
    "æ¹–å—å«è§†", "æµ™æ±Ÿå«è§†", "æ±Ÿè‹å«è§†", "ä¸œæ–¹å«è§†", "åŒ—äº¬å«è§†", "å¹¿ä¸œå«è§†",
    "æ·±åœ³å«è§†", "å››å·å«è§†", "æ¹–åŒ—å«è§†", "å±±ä¸œå«è§†", "æ²³å—å«è§†", "è¾½å®å«è§†",
    "å®‰å¾½å«è§†", "é™•è¥¿å«è§†", "å±±è¥¿å«è§†", "æ²³åŒ—å«è§†", "é»‘é¾™æ±Ÿå«è§†", "å‰æ—å«è§†",
    "å†…è’™å¤å«è§†", "æ–°ç–†å«è§†", "è¥¿è—å«è§†", "é¦™æ¸¯å«è§†", "é¦™æ¸¯å¼€ç”µè§†", "HOY TV",
    "ç¿¡ç¿ å°", "æ˜ç å°", "J2", "æ— çº¿æ–°é—»å°"
]

MAX_LINKS_PER_CHANNEL = 8
MIN_LINKS_PER_CHANNEL = 2
# ========================================================

def normalize_name(name):
    name = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9]", "", name)
    mapping = {
        "cctv1": "CCTV-1", "cctv2": "CCTV-2", "cctv3": "CCTV-3", "cctv4": "CCTV-4",
        "cctv5": "CCTV-5", "cctv5p": "CCTV-5+", "cctv5plus": "CCTV-5+", "cctv6": "CCTV-6",
        "cctv7": "CCTV-7", "cctv8": "CCTV-8", "cctv9": "CCTV-9", "cctv10": "CCTV-10",
        "cctv11": "CCTV-11", "cctv12": "CCTV-12", "cctv13": "CCTV-13", "cctv14": "CCTV-14",
        "cctv15": "CCTV-15", "cctv17": "CCTV-17", "cctv4k": "CCTV-4K",
        "å‡¤å‡°å«è§†": "å‡¤å‡°å«è§†", "å‡¤å‡°ä¸­æ–‡å°": "å‡¤å‡°ä¸­æ–‡å°", "å‡¤å‡°èµ„è®¯å°": "å‡¤å‡°èµ„è®¯å°",
        "æ¹–å—": "æ¹–å—å«è§†", "æµ™æ±Ÿ": "æµ™æ±Ÿå«è§†", "æ±Ÿè‹": "æ±Ÿè‹å«è§†", "ä¸œæ–¹": "ä¸œæ–¹å«è§†",
        "åŒ—äº¬": "åŒ—äº¬å«è§†", "å¹¿ä¸œ": "å¹¿ä¸œå«è§†", "æ·±åœ³": "æ·±åœ³å«è§†", "å››å·": "å››å·å«è§†",
        "æ¹–åŒ—": "æ¹–åŒ—å«è§†", "å±±ä¸œ": "å±±ä¸œå«è§†", "æ²³å—": "æ²³å—å«è§†", "è¾½å®": "è¾½å®å«è§†",
        "å®‰å¾½": "å®‰å¾½å«è§†", "é™•è¥¿": "é™•è¥¿å«è§†", "å±±è¥¿": "å±±è¥¿å«è§†", "æ²³åŒ—": "æ²³åŒ—å«è§†",
        "é»‘é¾™æ±Ÿ": "é»‘é¾™æ±Ÿå«è§†", "å‰æ—": "å‰æ—å«è§†", "å†…è’™å¤": "å†…è’™å¤å«è§†",
        "æ–°ç–†": "æ–°ç–†å«è§†", "è¥¿è—": "è¥¿è—å«è§†", "é¦™æ¸¯å«è§†": "é¦™æ¸¯å«è§†",
        "å¼€ç”µè§†": "é¦™æ¸¯å¼€ç”µè§†", "hoy": "HOY TV", "ç¿¡ç¿ ": "ç¿¡ç¿ å°", "æ˜ç ": "æ˜ç å°",
        "j2": "J2", "æ–°é—»å°": "æ— çº¿æ–°é—»å°"
    }
    for key in mapping:
        if key in name.lower():
            return mapping[key]
    return None

async def fetch_links(page, keyword):
    await page.goto(SEARCH_URL)
    await page.fill("input[name='q']", keyword)
    await page.click("input[type='submit']")
    await page.wait_for_timeout(3000)

    links = set()
    for _ in range(3):  # æœ€å¤šç¿»3é¡µ
        hrefs = await page.eval_on_selector_all("div#table a", "nodes => nodes.map(n => n.href)")
        for href in hrefs:
            if "watch" in href:
                try:
                    await page.goto(href, timeout=10000)
                    await page.wait_for_timeout(2000)
                    srcs = await page.eval_on_selector_all("video source, video", """
                        elements => elements.map(el => {
                            const src = el.src || el.children[0]?.src;
                            return src ? src.trim() : '';
                        }).filter(Boolean)
                    """)
                    links.update([s for s in srcs if s.startswith("http") and "m3u8" in s.lower()])
                except:
                    pass
                await page.go_back(timeout=10000)
                await page.wait_for_timeout(2000)
        try:
            next_btn = page.locator("a:has-text('Next')")
            if await next_btn.is_visible():
                await next_btn.click()
                await page.wait_for_timeout(3000)
            else:
                break
        except:
            break
    return list(links)

def test_url_speed(url):
    try:
        start = time.time()
        resp = requests.head(url, timeout=TIMEOUT, stream=True, headers={"User-Agent": "Mozilla/5.0"})
        if resp.status_code == 200:
            return time.time() - start
        else:
            resp = requests.get(url, timeout=TIMEOUT, stream=True, headers={"User-Agent": "Mozilla/5.0"})
            if resp.status_code == 200:
                return time.time() - start
    except:
        return float('inf')
    return float('inf')

async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()
        page = await context.new_page()

        all_channels = {}

        for channel in TARGET_CHANNELS:
            print(f"ğŸ” æœç´¢: {channel}")
            links = await fetch_links(page, channel)
            valid_links = []
            for link in set(links):
                delay = test_url_speed(link)
                if delay < float('inf'):
                    valid_links.append((link, delay))
                    print(f"âœ… æœ‰æ•ˆ: {link[:60]}... | å»¶è¿Ÿ: {delay:.2f}s")
            # æŒ‰é€Ÿåº¦æ’åº
            valid_links.sort(key=lambda x: x[1])
            # é™åˆ¶æ•°é‡
            selected = valid_links[:MAX_LINKS_PER_CHANNEL]
            if len(selected) >= MIN_LINKS_PER_CHANNEL:
                all_channels[channel] = selected
            else:
                print(f"âš ï¸  {channel} æœ‰æ•ˆæºä¸è¶³ {MIN_LINKS_PER_CHANNEL} ä¸ªï¼Œè·³è¿‡")
            await page.wait_for_timeout(10